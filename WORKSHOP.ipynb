{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJBEq3eB61oNmvP5bKj7lZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanipopuri2006/-Alpha-beta-pruning-of-Minimax-Search-Algorithm/blob/main/WORKSHOP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import random\n",
        "\n",
        "# ------------------------------\n",
        "# REPRODUCIBILITY\n",
        "# ------------------------------\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# ------------------------------\n",
        "# LOAD DATA\n",
        "# ------------------------------\n",
        "df = pd.read_csv(\"/content/income.csv\")\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# ------------------------------\n",
        "# DEFINE COLUMNS BASED ON YOUR FILE\n",
        "# ------------------------------\n",
        "categorical_cols = ['sex', 'education', 'marital-status', 'workclass', 'occupation']\n",
        "continuous_cols = ['age', 'hours-per-week']\n",
        "label_col = 'label'  # final target (0 or 1)\n",
        "\n",
        "# ------------------------------\n",
        "# ENCODE CATEGORICAL + LABEL\n",
        "# ------------------------------\n",
        "label_enc = LabelEncoder()\n",
        "df[label_col] = label_enc.fit_transform(df[label_col])\n",
        "\n",
        "cat_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    cat_encoders[col] = le\n",
        "\n",
        "# ------------------------------\n",
        "# SPLIT TRAIN/TEST (25k train, 5k test)\n",
        "# ------------------------------\n",
        "train_df = df.iloc[:25000]\n",
        "test_df = df.iloc[25000:30000]\n",
        "\n",
        "# ------------------------------\n",
        "# PREPARE ARRAYS\n",
        "# ------------------------------\n",
        "cat_train = np.stack([train_df[col].values for col in categorical_cols], axis=1)\n",
        "cat_test = np.stack([test_df[col].values for col in categorical_cols], axis=1)\n",
        "cont_train = np.stack([train_df[col].values for col in continuous_cols], axis=1)\n",
        "cont_test = np.stack([test_df[col].values for col in continuous_cols], axis=1)\n",
        "y_train = train_df[label_col].values\n",
        "y_test = test_df[label_col].values\n",
        "\n",
        "# ------------------------------\n",
        "# SCALE CONTINUOUS VARIABLES\n",
        "# ------------------------------\n",
        "scaler = StandardScaler()\n",
        "cont_train = scaler.fit_transform(cont_train)\n",
        "cont_test = scaler.transform(cont_test)\n",
        "\n",
        "# ------------------------------\n",
        "# CONVERT TO TENSORS\n",
        "# ------------------------------\n",
        "cat_train = torch.tensor(cat_train, dtype=torch.int64)\n",
        "cat_test = torch.tensor(cat_test, dtype=torch.int64)\n",
        "cont_train = torch.tensor(cont_train, dtype=torch.float)\n",
        "cont_test = torch.tensor(cont_test, dtype=torch.float)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# ------------------------------\n",
        "# DATA LOADERS\n",
        "# ------------------------------\n",
        "train_ds = TensorDataset(cat_train, cont_train, y_train)\n",
        "test_ds = TensorDataset(cat_test, cont_test, y_test)\n",
        "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
        "\n",
        "# ------------------------------\n",
        "# MODEL DEFINITION\n",
        "# ------------------------------\n",
        "class TabularModel(nn.Module):\n",
        "    def __init__(self, emb_dims, n_cont, hidden=50, p=0.4):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.ModuleList([nn.Embedding(categories, size)\n",
        "                                     for categories, size in emb_dims])\n",
        "        self.emb_drop = nn.Dropout(p)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "\n",
        "        n_emb = sum([size for _, size in emb_dims])\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(n_emb + n_cont, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.Dropout(p),\n",
        "            nn.Linear(hidden, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_cat, x_cont):\n",
        "        embeddings = [e(x_cat[:, i]) for i, e in enumerate(self.embeds)]\n",
        "        x = torch.cat(embeddings, 1)\n",
        "        x = self.emb_drop(x)\n",
        "        x_cont = self.bn_cont(x_cont)\n",
        "        x = torch.cat([x, x_cont], 1)\n",
        "        return self.layers(x)\n",
        "\n",
        "# ------------------------------\n",
        "# EMBEDDINGS\n",
        "# ------------------------------\n",
        "cat_sizes = [int(df[col].nunique()) for col in categorical_cols]\n",
        "emb_dims = [(size, min(50, (size + 1)//2)) for size in cat_sizes]\n",
        "\n",
        "# ------------------------------\n",
        "# MODEL, LOSS, OPTIMIZER\n",
        "# ------------------------------\n",
        "model = TabularModel(emb_dims, n_cont=len(continuous_cols), hidden=50, p=0.4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ------------------------------\n",
        "# TRAINING LOOP\n",
        "# ------------------------------\n",
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for cat_batch, cont_batch, y_batch in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(cat_batch, cont_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dl):.4f}\")\n",
        "\n",
        "# ------------------------------\n",
        "# EVALUATION\n",
        "# ------------------------------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred, y_true = [], []\n",
        "    test_loss = 0\n",
        "    for cat_batch, cont_batch, y_batch in test_dl:\n",
        "        outputs = model(cat_batch, cont_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        test_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        y_pred.extend(preds.tolist())\n",
        "        y_true.extend(y_batch.tolist())\n",
        "\n",
        "test_loss /= len(test_dl)\n",
        "accuracy = np.mean(np.array(y_pred) == np.array(y_true))\n",
        "print(f\"\\n✅ Test Loss: {test_loss:.4f}\")\n",
        "print(f\"✅ Test Accuracy: {accuracy*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5wB4_F0mGO5",
        "outputId": "73c94834-a677-4d5e-9f4a-c7102c5334cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (30000, 10)\n",
            "Columns: ['age', 'sex', 'education', 'education-num', 'marital-status', 'workclass', 'occupation', 'hours-per-week', 'income', 'label']\n",
            "Epoch 50/300, Loss: 0.2779\n",
            "Epoch 100/300, Loss: 0.2781\n",
            "Epoch 150/300, Loss: 0.2745\n",
            "Epoch 200/300, Loss: 0.2735\n",
            "Epoch 250/300, Loss: 0.2727\n",
            "Epoch 300/300, Loss: 0.2726\n",
            "\n",
            "✅ Test Loss: 0.2549\n",
            "✅ Test Accuracy: 88.38%\n"
          ]
        }
      ]
    }
  ]
}